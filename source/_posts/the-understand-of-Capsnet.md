	---
title: 【读论文】CapsNet
date: 2017-10-20 20:00:00
tags:
  - paper
categories: 读论文
toc: true

---
Hinton老师打算推翻这个世界

<!--more-->

最近所有科技自媒体都把目光放在了Hinton老师的一篇文章《Dynamic Roution Between Capsules》上，其实这篇文章在去年十一月份就发出来了，最近这篇文章的一作开源了论文的源代码，又引起了一波狂欢。科技自媒体的点在于Hinton提出了一种Capsules结构，来推翻自己提出的深度学习的基础--反向传播算法，并且直译为“胶囊网络”，简直不忍直视，说好的信达雅呢。但是我认为，这篇文章并没有完全的推翻BP算法，提出了一个新的角度来解决问题，来试图尝试解释神经网络。这篇文章的意义目前还不明显，因为Hinton老师确实给大家挖了一个大坑来填，论文中还有一些想法目前很粗糙。港真，可以水论文了。

### Capsules

论文提出了一种叫做capsules的结构，capsules是一个向量，表征了两个部分：
1. 其长度表征了某个实例（物体、视觉概念或者是他们的一部分）出现的概率
2. 其方向（长度无关的部分）表征了物体的某些图像属性（位置、颜色、方向、形状等）
既然capsules是一个向量，那么它是如何被应用呢？capsules作为一个向量，我们可以把它理解为feature map中的一个点。目前我们在神经网络中传递的数值都是标量，通过最后的Loss值，然后反向传播用来更新权重。capsules概念的提出让我们可以试图通过更容易理解的方式来构建神经网络，后面再提。在我看来，capsules的结构更像是传统神经网络中的一个层，因为论文后面提出的CapsNet也是以CNN开头的。后面的级联结构只是把传递的标量变为向量。

既然标量变成了向量，我们就不能用传统的非线性激活函数来处理，论文中提出了一个被称为**squashing**的非线性函数作为激活函数
【公式】【图】
这个函数的特点就是：
1. 值域在[0,1]之间，所以输出向量的长度可以表征某种概率，我们就可以用它做分类；
2. 函数单调递增，原来较长的向量收到鼓励，较短的向量收到抑制。
**squashing**函数实际上做到了对capsules的压缩和重新分布。

与此同时，层与层之间的连接部分也做出了相应的更改，变成了**线性组合+Dynamic Routing**的组合，这不就是变相的全连接层嘛，与全连接不同的是，这个组合针对的是一堆capsules。
【图】
Ci是可以进行更新的参数，这也是为什么被称为动态的原因所在，Routin的更新机制依赖于自己的输出，更新算法如下
【图】
更新算法很容易收敛，论文中认为3次迭代足矣。routing和其他算法一样存在过拟合，迭代次数过多能够提升准确率，但也会影响泛化能力。

### CapsNet

CapsNet的结构示意图如下，一共只有三层网络，***CNN + PrimaryCaps + DigitCaps***，论文最后的结果可以看到在MNIST数据集上可以媲美很深的卷积神经网络，然而，只是在MNIST上，其他数据集中表现并不好，所以还有许多坑要填。
【图】
首先还是一个卷积核为9x9的卷积层，卷积核的尺寸很大，应该是为了更充分的感受到原始图像。为什么不从一开始就用capslues？有一种说法是capslues并不能从像素级别提取特征，需要一个浅层的卷积用来提取特征，然后再进行进一步的归纳特征，好像也是有那么个意思。

然后是PrimaryCaps层，在卷积层中用8个不同的卷积核组成，一共是32个feature map。

最后一层是DigitCaps层，也就是输出层，一组10个标准的Capsules，每一个capsule代表一个数组，输出向量元素的个数是16个。

只有在PrimaryCasules到DigitCaps中使用了上文所说的Dynamic routing。

### Reconstruct

看到这里，会觉得这也没有啥很特殊的，也就是一个特殊的结构而已嘛，接下来的工作就是真正神奇的地方了。

Hinton一直坚持的一个理念就是，一个好的robust的模型，一定能够由重构能力，能够通过提取出的特征表示还原出原始图像，我们才能够说这个模型能够很好的表示特征。

论文中把DigitCaps的数据直接扔到全连接层进行重构，如下图的结构。
【图】
论文中通过人为扰动capsule的输出向量能够成功的对图像的特征进行有规律的改变，这是以往的网络结构，包括GAN中很难做到的一点。
【图】
甚至对于重叠的数字，都会由很强的重构能力。
[tu]
我们有理由相信，Capsules结构中包含了我们能够理解的符合人类习惯的信息，我们可以通过人为的干扰改变重构信息，我们就多了一丝机会去理解神经网络。

### Discussion

至于这项工作是否值得像科技媒体那样的震惊，我认为目前还欠火候。首先并没有完全的抛弃反向传播算法，第一层的CNN的参数还需要更新，其次，Dynamic Routing的参数量问题是一个不得不考虑的事情。Hinton大神表示，坑已经挖好，你们可以灌水了。至于这个究竟是不是真的会带来改变，只能说，时间，时间会告诉我答案。


