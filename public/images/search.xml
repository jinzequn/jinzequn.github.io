<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[我们随便谈谈[18.08.12]]]></title>
    <url>%2F2018%2F08%2F12%2Fessay-0812%2F</url>
    <content type="text"><![CDATA[二零一八年八月十二日，我们随便谈谈。 来北京差不多要整整一年了，目前的生活状态是来之前根本没有想过的吧，这一年所有的事情在飞速的发展，事情的发展已经远远的超出了我的预期，从没有窗户的小平房，搬到了上下班通勤两个小时的天通苑，又搬到了距离公司只有二十多分中的清河。原本两到三年的规划，在这一年的时间内迅速的实现，速度快的真的有些难以接受。 走的太快，以至于还没想好接下来怎么办。这种看不到方向的感觉让我倍感焦虑，逼着你不得不做出点什么接下来的规划，但是又真的没有什么规划，也没有什么想法和安排，无所适从。所以，在没有大方向的背景下，给自己安排一些小任务，总结一下接下来的任务和安排。 工作还是要做，现在学的东西太少，多读代码。 硕士毕业论文要写了，目前还有两个月的时间，一个月内写出初始版本，然后回学校给老师修改。 日常学英语，雅思这件事儿需要想一想，然后安排上 最近看到，听到一些事情，之前我还会想，这个世界还会好么？现在我不会这么想了，这个世界本来就不好，这个世界的好就是宣传组织凭空臆想出来用来安稳社会的，这个世界就是这个样子，就是你看到的样子，而不是你想象出来的样子。这是没办法改变的，本身就是一个动物世界，游戏规则就是这样，要么服，要么死。我不想因为这点破事儿去死，所以我选择活着。好的东西和不好的东西都在那里，要是非要盯着不好的东西看，这不是和自己过不去么？ 有种突然想明白的感觉，平平凡凡的活着多好，我的教育体系里没有被灌输非要出人头地，风风光光的价值观。我爸妈的对我也一直没有这样的要求，反而一直以来给我灌输的思想就是差不多就行，这很好，但很无趣。前两天看了一个小米的纪录片，雷军有句话说，失败就在一夜之间。我觉得这么说丝毫不夸张，我扪心自问，我能承受这样的压力么？我希望过这样的生活么？我觉得不能。成功其实也很简单，任何一个人，要是拼了命，没啥干不了的，但是你会失去很多，首当其冲就是家庭。创业是个很艰辛的过程，九死一生，我会不会创业不好说，可能需要看我是不是真的缺钱，是不是真的无聊。 就说到这儿吧，最近真的是太无趣了，我对如何去生活这件事儿真的很不了解，事业是事业，生活要有生活，我要学着生活，这样才不会被生活抛弃。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我们随便谈谈[18.02.01]]]></title>
    <url>%2F2018%2F02%2F01%2Feassy-0201%2F</url>
    <content type="text"><![CDATA[二零一八年二月一日，我们随便谈谈。 想起之前看过的一本书心理学家丹尼尔·卡尼曼《思考，快与慢》，其中阐述了一个很重要的概念，人类的潜意识。潜意识有多重要呢，有这么个观点是说，人做出的大多数决定都是源于潜意识。我们遇到一个选择，第一瞬间潜意识就替我们做好了选择，而后我们所有的理性思考都是为了给潜意识服务，试图证明潜意识的正确性。但其实潜意识做出的选择是不是最优的呢？无关紧要。选择的好坏是个伪命题，没有人能够确认之后发生的事儿。 这样的可怕之处在于什么地方呢？潜意识替我们做出选择，并且潜意识能够被周围的环境所影响，我们的选择又靠潜意识完成，也就是说，我们所做出的选择是能够被影响，而我们束手无策。群体效应是最为明显的潜意识决定行为的实例之一。我现在在想，我写下这篇文章，是不是我潜意识里面确认了这个效应，才会用理性去分析它。 理性和感性的矛盾一直都在，无数的鸡汤告诉我们，一定要充满理性，不要被感性占据了上风。私以为这句话毫无意义，我们这个世界就是一个感性的世界，什么时候需要理性呢？也许是我写代码的时候，也许是你搞科研的时候。然而你若妄想用理性去解决生活的问题，是不可能的。人是一个感性的物种，只有机器才能遵守理性的约束，一直执行下去。只要牵扯到与人有关系的任何事情，都不要妄想去用理性的方法解决。 我们很容易被潜意识所控制，并且几乎无法被察觉。可能你看到的某个观点，刚开始并不赞同，但是你的潜意识是认可的，你就可能会做出完全不同的决定。我们能做的就是用大量的数据来影响潜意识，不至于做出的决定过于偏颇。读更多的书，听更多的观点，和更多的人进行交流讨论，听听正反的观点，锻炼自己独立思考的能力，好让自己的潜意识能够靠谱一点。 这个世界是个理性无意义的世界，所有的认知都出于感性。我所看到世界，我所感受到的世界，是只属于我的世界，和任何人的都不同，或许也只存在这一个只属于我的世界。Whatever,毫无意义。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈Deconv和Unpooling]]></title>
    <url>%2F2018%2F01%2F28%2Fdeconv-and-unpool%2F</url>
    <content type="text"><![CDATA[Deconv和Unpooling作为标准操作却很少被人提及 在深度学习分类检测等任务中，大部分操作都是在下采样(downsampling)，因为我们输入高维的图像，然后输出是低维的坐标信息或者是分类信息，所以我们需要下采样来减少维度，但是在一些特殊的任务中，比如生成或者是语义分割的时候，我们最终的维度跟输入的图像维度相同甚至要更大，所以我们需要一些上采样(upsampling)的操作，Deconv和Unpool就是这样的操作，能够增加维度信息，达到升维的效果。 UnpoolingUnpooling作为一种上采样的方法，与pooling看起来像是相反的操作，我们有三种方法，第一种是Nearest Neighbor，就是把相同的数据复制４个达到扩大四倍的效果，第二种是”Bed of Nails”，把数据防止在对应位置的左上角，然后其余的地方补0，如下图所示。 第三种方法是Max Unpooling，对于一部分网络模型来讲，上采样和下采样的结构往往是对称的，我们可以在下采样做Max Pooling的时候记录下来最大值所在的位置，当做上采样的时候把最大值还原到其对应的位置，然后其余的位置补0,如下图所示。 这样可以最大限度的还原信息。 Deconv对于Deconv这个名字，学界一直是有争议的。现在我们用的更多的是Transpose Convolution. Deconv听起来像是卷积的逆，但是实际上并不是，这里只是一个上采样的方法，用的基础操作还是卷积，用一个例子说明，我们想要把一个2x2的输入上采样至4x4，如下图所示。 我们仍用3x3的卷积核，值得注意的是，我们在输入的时候做了stride=2，所以我们的输出扩大了2倍，我们可以很清楚的计算出宽高，在3x3的例子里面经过Transpose Convolution的这九个数就是，输入的一个数分别乘以卷积核，输出重叠的部分相加。我们用一维的例子来说明更清楚。 相互重叠的地方就sum，这就是为什么我们用Deconv这个操作做生成模型的时候，生成出来的图像会有棋盘格效应，为了解决这个问题，近期一部分学者通过设置卷积核的大小和步长来规避掉重叠部分，也不失为一种方法。 所以为什么说Transpose Convolution这个名字更合适呢？在caffe中，我们不去设计一个滑动的卷积核，而是通过将数据扩展为一维向量，卷积核扩展为一个大矩阵，直接做矩阵乘法来获得输出，这样在GPU中可以最大限度的加速，而Transpose Convolution的操作就是把卷积核的大矩阵做上下左右翻转，然后再左乘输入向量，所以说被称为Transpose Convolution更为合适。详细的证明再此不在复述，有兴趣的同学可以查一下相关资料。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blog+github备份和恢复]]></title>
    <url>%2F2018%2F01%2F24%2Fgithub-hexo%2F</url>
    <content type="text"><![CDATA[换了新电脑以后如何恢复BLOG 在用hexo搭建博客的时候需要备份一下，这样电脑出问题的时候或者换新的电脑以后才能快速的恢复blog环境。 首先讲一讲搭建的流程 创建仓库，名字必须是 用户名.github.io 创建两个分支，master和hexo 设置hexo为默认分支 使用git clone git@github.com:(仓库名)克隆到本地 在文件夹下打开终端，分支显示为hexo，依次执行npm install hexo \ hexo init \ npm install \ npm install hexo-deploy-git 修改 _config.yml中的deploy参数，分支应为master 依次执行git add . , git commit -m “…” , git push origin hexo 提交网站相关资料 执行hexo g -d 部署blog 对于日常修改博客的时候，应该首先用git add . , git commit -m “…” , git push origin hexo 将内容保存至github中，然后才用hexo g -d部署博客 更换电脑以后如何恢复呢？ 使用 git clone -b hexo git@github.com: ….将仓库克隆到本地。 然后在文件夹下执行: npm install hexo / npm install / npm install hexo-deployer-git (不需要hexo init) 可能会遇到一些问题，命令行里面也会提示你到相应的地方去找解决方案，按照提示做就行，再不济google一下。 完美！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我们随便谈谈[17.12.05]]]></title>
    <url>%2F2017%2F12%2F05%2Fessay-1205%2F</url>
    <content type="text"><![CDATA[二零一七年十二月五日，我们随便谈谈。 早该谈一下最近发生的一些事儿，由于前些日子被情绪冲的七零八落，想等平静下来再思考一下，然而，当我平静下来思考的时候，并没有能够想明白。最近发生的很多事儿确实刷新了我对整个世界的理解。从杭州保姆纵火，江歌妈妈见刘鑫，到幼儿园虐童，豫章书院，驱逐低端人口。好像最近国家对社会舆论的引导失去了控制。以至于如此骇人听闻的事情被媒体报道了出来。人性自来邪恶，媒体的报道是真是假没人能说的清楚，我一直想搞清楚的是什么人为什么要把这些事情拿出来引导舆论。 半个月前，北京大兴火灾烧死19个人，北京随即在全市范围内开展排查工作，拆除违章建筑，当然，我住的公寓也被拆除了，其实我的公寓的管理还算规范，检查工作还算严格，然而被一刀切拆掉。拆公寓这件事我是能理解的，有的公寓脏乱差，出事儿是迟早的事儿。我不能理解的是11月22号晚上我们接到通知要求11月24号早上搬出，也就是说只有一天的时间，我们要收拾行李找到新的地方住。11月23号那天是星期四，我们也只能请假去找房子。这个事儿办的不能说有什么错，只是不地道。有多少人短时间内根本找不到房子住，那几天晚上温度都在零下五度左右。两天后，官方召开发布会也只是草草一句“有的单位操之过急”，两天，大部分手足无措的人只能放弃工作回老家了吧。半个月后，谁还记得这件事儿呢？ 大家都很健忘，所以压制舆论很简单。让这些个话题消失在公共的视野中，只需十天半月，谁还会记得？制造舆论也很简单，人云亦云是最好的方法。我们的情绪就这样被人玩弄着，身在群体像陷在沼泽中一样。前段时间一直在看一本书叫《乌合之众》，讲的就是群体效应，在看完书以后的这一系列事情中，群体效应体现的淋漓尽致。虽然能感觉到是有人在故意引导舆论，但是，身在其中，我也无法脱离开来理性思考。呵， 可怜的人！ 我看到的世界就像是被人精心安排过的需要被我看到的世界，我对我能接受到的信息无法选择，一想到这里我就感觉脊背一阵凉风吹过。无论是宗教还是政权，都是统治人心的工具，控制一个群体的思想才能做到真正的控制，就比如说传销进行洗脑，军队营造同生共死的团结气氛，都是为了对其思想进行控制。而把人融入群体，就能够轻易的控制群体中的每一个人，因为群体的感性会被无限的放大而理性会被无限的压制。还记得我们上高中的时候，有一个感恩的演讲，最后主持人在台上痛哭流涕，台下同学们也痛苦流涕，然而，回家独自听这种演讲时，却毫无感觉。群体的力量就是如此的可怕。 高考时八股文写多了，现在写什么都像是议论文。高老师跟我说过一句话让我印象很深：“人啊，别活太明白了”，没错，什么都想知道，但知道这个世界真的发生了什么，还会觉得活着是有意义的么？ 生而为人，真的很抱歉。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow分布式部署【多机多卡】]]></title>
    <url>%2F2017%2F12%2F05%2Ftensorflow-distribute%2F</url>
    <content type="text"><![CDATA[让TensorFlow们飞一会儿 前一篇文章说过了TensorFlow单机多卡情况下的分布式部署，毕竟，一台机器势单力薄，想叫兄弟们一起来算神经网络怎么办？我们这次来介绍一下多机多卡的分布式部署。 其实多机多卡分布式部署在我看来相较于单机多卡分布式更容易一些，因为一台机器下需要考虑我需要把给每个device分配哪些操作，这个过程很繁琐。多台机器虽然看起来更繁琐，然而我们可以把每一台机器看作是一个单卡的机器，并且谷歌爸爸已经把相对复杂的函数都给封装好了，我们直接拿来用就行。为什么这么说呢？我们首先介绍两个概念In-graph模式和Between-graph模式： In-graph模式: 这个模式跟单机单卡是差不多的，我们需要把不同的节点分配给不同的设备，比如说我让某台机器的某个GPU做一部分卷积，另外某台机器的某个GPU做另外一部分卷积，这样大家都有活干。想象总是美好的，在实际情况中会出现什么问题呢？数据搬移量太大，会有相当一部分时间耗费再数据搬移之下，Tensor翻山越岭，穿过网线，来到一个设备中，凳子还没坐热，有出发去另外一个设备。在大量训练数据的情况下，这种方法往往是不可取的。 Between-graph模式： 这个模式下每一个设备都相当于独立的完成整个卷积神经网络的操作，只是在开始时从参数服务器中取到参数，然后结束的时候送回参数。所以除了chief节点以外，所有人都可以在训练过程中随时退出，随时加入，但是刚开始时，大家都要响应一下chief节点的号召。这样显然更合理一点，在大量数据的情况下我们会选用这个方法，下面的代码也会以Between-graph模式作为例子。 上文提到在Between-graph模式下我们需要在训练过程中从参数服务器中获取参数，那么问题来了，什么是参数服务器？接下来我们再引入两个概念（忍一下忍一下，很简单）： 参数服务器：顾名思义，参数服务器嘛，保存参数用的服务器，简称ps(paramEter severs)。参数服务器可以不止一个，如果参数量过大的话，我们可以多叫几台计算机过来充当参数服务器，用来更新参数。 工作服务器： 顾名思义，工作服务器嘛，干活的，简称worker。一般为GPU们，能够进行快速并行计算的设备，它可以从参数服务器中把参数荡下来，然后计算出来以后在传上去。 基础的介绍完了，同样的每个工作模式下都会有参数同步更新和异步更新，下面放张就是那么个意思的图(现在没图都不好写博客了…)。 好，总结一下，我们可以设置多个参数服务器(ps)用来存储更新参数，同时我们也可以设置多个工作服务器(worker)用来进行计算。这样就组成了一个多机多卡分布式的Tensorflow开发环境。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow分布式部署【单机多卡】]]></title>
    <url>%2F2017%2F12%2F01%2Ftensorflow-mulit-gpus%2F</url>
    <content type="text"><![CDATA[让TensorFlow飞一会儿 面对大型的深度神经网络训练工程，训练的时间非常重要。训练的时间长短依赖于计算处理器也就是GPU，然而单个GPU的计算能力有限，利用多个GPU进行分布式部署，同时完成一个训练任务是一个很好的办法。对于caffe来说，由于NCCL的存在，可以直接在slover中指定使用的GPU。然而对于Tensorflow，虽然Contrib库中有NCCL，但是我并没有找到相关的例子，所以，还是靠双手成就梦想。 原理简介TensorFlow支持指定相应的设备来完成相应的操作，所以如何分配任务是很关键的一环。GPU擅长大量计算，所以整个Inference和梯度的计算就交给GPU来做，更新参数的小事情就交给CPU来做。这就比如校长要知道整个年级的平均成绩，就把改卷子的任务分配给每个班的老师，每个班的老师批改完卷子以后，把各自班级的成绩上交给校长，校长计算个平均数就行。在这里，校长就是CPU，每个班级的老师就是GPU。 下面放出一张图来说明问题。 我们可以清楚的看到CPU中保存变量，GPU们计算整个model和gradients，然后把得到的梯度送回CPU中，CPU计算各个GPU送回来梯度的平均值作为本次step的梯度对参数进行更新。从图中我们可以看到只有当所有的GPU完成梯度计算以后，CPU才能求平均值，所以，整个神经网络的迭代速度将取决于最慢的一个GPU，这也就是同步更新。那能不能异步更新呢？当然是可以的把更新参数这个操作也放回到GPU上，但是异步更新会造成训练不稳定，有的快有的慢，你说到底听谁的… 在上图中我们可以看到有几个关键点需要注意： 在CPU上定义变量 在GPU上分别定义model和gradients操作，得到每个GPU中的梯度 又回到CPU中计算平均平均梯度，并进行参数更新 Talk is cheap, show me the code!! 好，下面放代码。 示例代码示例代码分如下几个部分： 读入数据 在cpu中定义变量 搭建Inference 定义loss 定义训练过程 读入数据由于是在不同的GPU上进行运算，所以我们采用TF官方的数据格式tfrecords作为输入，tfrecords的MNIST数据集格式可以在网上很轻易的找到。读入数据的时候我们就用标准的tfrecords数据集读入的格式。12345678910111213141516def read_and_decode(filename_queue): reader = tf.TFRecordReader() _, serialized_example = reader.read(filename_queue) features = tf.parse_single_example( serialized_example, # Defaults are not specified since both keys are required. features=&#123; 'image_raw': tf.FixedLenFeature([], tf.string), 'label': tf.FixedLenFeature([], tf.int64), &#125;) image = tf.decode_raw(features['image_raw'], tf.uint8) image.set_shape([IMAGE_PIXELS]) image = tf.cast(image, tf.float32) * (1. / 255) - 0.5 label = tf.cast(features['label'], tf.int32) return image, label 这段函数会返回一个图像和标签，我们需要按照Batch的方式读入12345678910111213141516def inputs(train, batch_size, num_epochs): if not num_epochs: num_epochs = None filename = os.path.join(FLAGS.data_dir, TRAIN_FILE if train else VALIDATION_FILE) with tf.name_scope('input'): filename_queue = tf.train.string_input_producer( [filename], num_epochs=num_epochs) image, label = read_and_decode(filename_queue) images, sparse_labels = tf.train.shuffle_batch( [image, label], batch_size=batch_size, num_threads=2, capacity=1000 + 3 * batch_size, min_after_dequeue=1000) return images, sparse_labels 到这里我们可以读入batch图像和标签。 在CPU中定义变量我们需要把weight和biases定义在CPU中，以便进行参数的更新。注意12345678910111213141516```Pythondef _variable_on_cpu(name, shape, initializer): &quot;&quot;&quot;Helper to create a Variable stored on CPU memory. Args: name: name of the variable shape: list of ints initializer: initializer for Variable Returns: Variable Tensor &quot;&quot;&quot; with tf.device(&apos;/cpu:0&apos;): dtype = tf.float32 var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype) return var 构建Inference构建Inference采用的的是卷积神经网络的架构，需要注意的是初始化的时候需要将变量定义在CPU中。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576def inference(images): """Build the MNIST model. Args: images: Images returned from MNIST or inputs(). Returns: Logits. """ x_image = tf.reshape(images, [-1, 28, 28, 1]) # conv1 with tf.variable_scope('conv1') as scope: kernel = _variable_on_cpu('weights',shape=[5,5,1,32], initializer = tf.truncated_normal_initializer(stddev=5e-2)) biases = _variable_on_cpu('biases', [32], tf.constant_initializer(0.0)) conv = tf.nn.conv2d(x_image, kernel, strides=[1, 1, 1, 1], padding='SAME') pre_activation = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(pre_activation, name=scope.name) # pool1 pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1') # conv2 with tf.variable_scope('conv2') as scope: kernel = _variable_on_cpu('weights',shape=[5,5,32,64], initializer = tf.truncated_normal_initializer(stddev=5e-2)) conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME') biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1)) pre_activation = tf.nn.bias_add(conv, biases) conv2 = tf.nn.relu(pre_activation, name=scope.name) # pool2 pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2') # local3 with tf.variable_scope('local3') as scope: # Move everything into depth so we can perform a single matrix multiply. reshape = tf.reshape(pool2, [-1, 7 * 7 * 64]) dim = reshape.get_shape()[1].value weights = _variable_on_cpu('weights',shape=[dim,1024], initializer = tf.truncated_normal_initializer(stddev=0.04)) biases = _variable_on_cpu('biases', [1024], tf.constant_initializer(0.1)) local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name) # local4 with tf.variable_scope('local4') as scope: weights = _variable_on_cpu('weight',shape=[1024,10], initializer = tf.truncated_normal_initializer(stddev=0.04)) biases = _variable_on_cpu('biases', [10], tf.constant_initializer(0.1)) local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name) # linear layer(WX + b), # We don't apply softmax here because # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits # and performs the softmax internally for efficiency. with tf.variable_scope('softmax_linear') as scope: weights = _variable_on_cpu('weight',[10,10], initializer = tf.truncated_normal_initializer(stddev=1 / 192.0)) biases = _variable_on_cpu('biases', [10], tf.constant_initializer(0.0)) softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name) return softmax_linear 定义Loss定义loss的时候和单GPU的形式不同，因为我们不仅要定义损失函数，还要定义每个GPU的损失函数值和其梯度，最后再计算平均梯度。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def loss(logits, labels): """Add L2Loss to all the trainable variables. Add summary for "Loss" and "Loss/avg". Args: logits: Logits from inference(). labels: Labels from distorted_inputs or inputs(). 1-D tensor of shape [batch_size] Returns: Loss tensor of type float. """ # Calculate the average cross entropy loss across the batch. labels = tf.cast(labels, tf.int64) cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits( labels=labels, logits=logits, name='cross_entropy_per_example') cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy') tf.add_to_collection('losses', cross_entropy_mean) # The total loss is defined as the cross entropy loss plus all of the weight # decay terms (L2 loss). return tf.add_n(tf.get_collection('losses'), name='total_loss')def tower_loss(scope): """Calculate the total loss on a single tower running the MNIST model. Args: scope: unique prefix string identifying the MNIST tower, e.g. 'tower_0' Returns: Tensor of shape [] containing the total loss for a batch of data """ # Input images and labels. images, labels = inputs(train=True, batch_size=FLAGS.batch_size, num_epochs=FLAGS.num_epochs) # Build inference Graph. logits = inference(images) # Build the portion of the Graph calculating the losses. Note that we will # assemble the total_loss using a custom function below. _ = loss(logits, labels) # Assemble all of the losses for the current tower only. losses = tf.get_collection('losses', scope) # Calculate the total loss for the current tower. total_loss = tf.add_n(losses, name='total_loss') # Attach a scalar summary to all individual losses and the total loss; do # the same for the averaged version of the losses. if FLAGS.tb_logging: for l in losses + [total_loss]: # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU # training session. This helps the clarity of presentation on # tensorboard. loss_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', l.op.name) tf.summary.scalar(loss_name, l) return total_lossdef average_gradients(tower_grads): """Calculate average gradient for each shared variable across all towers. Note that this function provides a synchronization point across all towers. Args: tower_grads: List of lists of (gradient, variable) tuples. The outer list is over individual gradients. The inner list is over the gradient calculation for each tower. Returns: List of pairs of (gradient, variable) where the gradient has been averaged across all towers. """ average_grads = [] for grad_and_vars in zip(*tower_grads): # Note that each grad_and_vars looks like the following: # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN)) grads = [] for g, _ in grad_and_vars: # Add 0 dimension to the gradients to represent the tower. expanded_g = tf.expand_dims(g, 0) # Append on a 'tower' dimension which we will average over below. grads.append(expanded_g) # Average over the 'tower' dimension. grad = tf.concat(grads, 0) grad = tf.reduce_mean(grad, 0) # Keep in mind that the Variables are redundant because they are shared # across towers. So .. we will just return the first tower's pointer to # the Variable. v = grad_and_vars[0][1] grad_and_var = (grad, v) average_grads.append(grad_and_var) return average_grads 定义训练过程训练过程的需要注意把不同的环节放在不同的devices下面。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899def train(): with tf.Graph().as_default(), tf.device('/cpu:0'): # Create a variable to count the number of train() calls. This equals # the number of batches processed * FLAGS.num_gpus. global_step = tf.get_variable( 'global_step', [], initializer=tf.constant_initializer(0), trainable=False) # opt = tf.train.MomentumOptimizer(lr,0.9,use_nesterov=True,use_locking=True) opt = tf.train.MomentumOptimizer(INITIAL_LEARNING_RATE,0.9,use_nesterov=True,use_locking=True) # Calculate the gradients for each model tower. tower_grads = [] with tf.variable_scope(tf.get_variable_scope()): for i in xrange(FLAGS.num_gpus): with tf.device('/gpu:%d' % i): with tf.name_scope( '%s_%d' % (TOWER_NAME, i)) as scope: # Calculate the loss for one tower of the CIFAR model. # This function constructs the entire CIFAR model but # shares the variables across all towers. loss = tower_loss(scope) # Reuse variables for the next tower. tf.get_variable_scope().reuse_variables() # Retain the summaries from the final tower. summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope) # Calculate the gradients for the batch of data on this # MNIST tower. grads = opt.compute_gradients(loss, gate_gradients=0) # Keep track of the gradients across all towers. tower_grads.append(grads) # We must calculate the mean of each gradient. Note that this is the # synchronization point across all towers. grads = average_gradients(tower_grads) train_op = opt.apply_gradients(grads, global_step=global_step) # The op for initializing the variables. init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) # Start running operations on the Graph. allow_soft_placement must be # set to True to build towers on GPU, as some of the ops do not have GPU # implementations. sess = tf.Session(config=tf.ConfigProto( allow_soft_placement=True, log_device_placement=FLAGS.log_device_placement)) sess.run(init_op) # Start input enqueue threads. coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) try: step = 0 while not coord.should_stop(): start_time = time.time() # Run one step of the model. The return values are # the activations from the `train_op` (which is # discarded) and the `loss` op. To inspect the values # of your ops or variables, you may include them in # the list passed to sess.run() and the value tensors # will be returned in the tuple from the call. _, loss_value = sess.run([train_op, loss]) duration = time.time() - start_time # assert not np.isnan( # loss_value), 'Model diverged with loss = NaN' # Print an overview fairly often. if step % 100 == 0: num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus examples_per_sec = num_examples_per_step / duration sec_per_batch = duration / FLAGS.num_gpus format_str = ( '%s: step %d, loss = %.2f (%.1f examples/sec; %.3f ' 'sec/batch)') print(format_str % (datetime.now(), step, loss_value, examples_per_sec, sec_per_batch)) step += 1 except tf.errors.OutOfRangeError: print('Done training for %d epochs, %d steps.' % ( FLAGS.num_epochs, step)) finally: # When done, ask the threads to stop. coord.request_stop() # Wait for threads to finish. coord.join(threads) sess.close() 最后就可以调用Train()函数进行训练了。训练函数分配GPU的时候有for循环，所以可以支持不同数量的GPU。 单机多卡服务器进行深度学习的训练，构建代码比较复杂，并且需要手动分配devices，相比于NCCL的高级库好的一点就是可以针对不同的任务进行定制化的分配，以实现最大程度的优化，工作量比较大，效果也非常好。搭建的时候需要平衡一下效率和开发速度。后续还会尝试多机多卡的情况，目前还在尝试。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFLow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫谈卷积神经网络]]></title>
    <url>%2F2017%2F11%2F24%2FTalk-about-CNN%2F</url>
    <content type="text"><![CDATA[现在各种复杂的神经网络模型当然不是突然出现在世界上的，一定有一个演进更新的过程。 在图像分类领域，从古老的Lenet到如今的Resnet，这一路走来，人们在改进卷积神经网络模型方面发现了什么问题，做了哪些思考，又进行了什么改进。在这篇文章中，我会简单的介绍一下各个经典卷积神经网络模型都做出了什么改进，仅仅是主要思想，不涉及论文细节。 通俗意义上的卷积神经网络来自于1998，Yann LeCun、Yoshua等人搭建了一个多层神经网络模型，并且发现卷积操作对图像的特征提取有一个很好的表现，于是将卷积操作引入神经网络中，这也就是我们现在说的卷积神经网络。但是当时并没有引起很大的反响，因为卷积神经网络没有严谨的数学推理，并且在当时的无论是数据量和计算能力都无法撑起这个算法。这个现象我们通常叫做生不逢时。LeNet的结构如下图所示，简单清晰明了，来自1998年的卷积神经网络的基本组件包括卷积层、Pooling层和FC层我们现如今还在一直沿用。 时间来到2012年，不知是否是冥冥中的巧合，在传说中的世界末日的这一年，我们迎来了真正意义上的卷积神经网络。早在2009年，斯坦福计算机视觉实验室，发起了一项比赛，也就是我们后来说的ImageNet，想要推得动计算机视觉的发展，在2012年来自Hinton及其学生Alex拿出了他们精心炼制的AlexNet卷积网络模型将图像分类的错误率降低了百分之十几，碾压式的拿到了这项比赛的冠军，而后就是深度学习的兴起，计算机视觉领域，深度学习模型埋掉了大部分经典理论。AlexNet如下图所示，不要问我为什么这个图被截掉了一部分，原来论文中就是这么画的。 由于当时GPU容量和计算能力的限制，Alex把卷积得到的特征图分成两份，分别送入两个GPU中进行训练。分别进行卷积这件事儿并不仅仅这么简单，后来一部分工作就是建立在分别卷积上进行的，我们称之为Group Convolution。AlexNet有大量的参数,这是因为它的卷积核特别大，存在11x11、7x7等大型的卷积核，这会导致大量的存储成本和计算成本。所以在2014年来自牛津大学的计算机视觉组VGG提出了他们的想法。 VGG网络的核心思想就是取消掉大型的卷积核，用叠加的3x3的卷积核代替5x5和7x7的卷积核,这样一来，卷积核的大小减少了，模型大小也减少了，同时增加了网络深度，提高了模型的非线性程度，也就增加的模型的表达能力。通过实验提出了两种模型结构分别是VGG-16和VGG-19，这两个神经网络模型也是目前比较稳定的神经网络模型，应用非常广泛。 VGG论文中用3x3的卷积核代替5x5和7x7的卷积核，那么在一个卷积层中用多大的卷积核比较合适呢？ 1x1? 3x3? 5x5?还是直接用pooling更好？Google说交给神经网络决定吧，所以Google提出了一个叫做Inception的结构。 Inception网络的包含了不同尺度的卷积核分别对输入层进行卷积，然后把卷积得到的feature maps叠加到一起，注意，我们这里说的是叠加，而不是数值的相加，就像三明治一样叠加在一起。哪个尺度的卷积核在这一层中起作用，交给神经网络决定。在这个结构中我们看到了每一个分支都有一个1x1的卷积核存在，它的作用是降维，比如说将128个feature maps 降维到64个feature maps，这个操作也有一种说法叫做Bottleneck。关于降维这件事很玄，有人说可以降低参数量，有人说可以提取出要特征，类似于PCA的效果，嗯，说不清说不清，谁也证明不出来。 既然一个卷积层的卷积核尺度交给了神经网络来决定，那么需要多少个卷积层也交给神经网络来决定吧。2015年微软亚洲研究院孙剑博士小组带来了震惊世界的残差网络。神经网络中引入了一个很重要的概念叫做shortcut connection,残差块结构形式如下图左上角所示。 让我们换个姿势看一下这个结构，上如的左下角是三个残差块堆叠出来的网络。每一个二岔路口我都可以选择直接跳过去或者是经过这个卷积层。这还是看不出来什么，我们再换个姿势看一下，右下角列出了一部分情况，这次能看的很清楚了。这就是和一个多人投票系统嘛。是由不同个残差块组成的多人投票系统。在ImageNet中得到冠军的那个残差网络模型一共有152层，而后在所有的演讲中微软亚洲研究院一直在强调的一点是网络深度，然而每次真正有多少的卷积层参与的预测呢，我们看一下右上角的图，实际上被激活的卷积层大体也是十多二十层。这个结构真正解决了多层卷积层带来的梯度弥散的情况么？在我看来并没有。 后来又出现了Inception和Resnet的组合网络，就不一一赘述了，在图像分类领域，卷积神经网络的演进也就谈的差不多了，现在看来这几年的工作还是很有成效的，总体趋势是想着轻量化和高准确率演进，ImageNet已经终止了，未来是否还有如此令人激动的学术热潮不得而知，可能会有更多的人把精力投入到深度学习的落地当中去吧，希望人们的生活真正的通过深度学习变得更加美好。]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【读论文】LapSRN]]></title>
    <url>%2F2017%2F10%2F24%2Fthe-understand-of-LapSRN%2F</url>
    <content type="text"><![CDATA[这篇论文只强调一个事儿，我这个超分辨率又快又好！ 论文全文名称为《Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks》，翻译过来就是基于深度拉普拉斯金字塔网络又快又好的图像超分辨率，emmmmmm…，这论文名字还是挺勇敢。我们后文中简称它为LapSRN。 贴出来它的网站（对，它有一个主页）：http://vllab.ucmerced.edu/wlai24/LapSRN/ 竟然还拿我赫敏做例子（不要翻，这是一个GIF），这篇论文不看是不行了。（但是不应该是倍数越大越清晰么。。） 冷静，冷静一下。这篇论文怎么个不一样呢？我们来看一下摘要部分，现有的卷积神经网络需要大量的参数以及巨大的计算量来生成产分辨率结果，根本做不到实时。在这篇论文中，我们提出了一种深度拉式金字塔模型的超分辨神经网络，从而达到了一个又快又好的结果。从多层金字塔结构中对超分辨进行重建，取代了以前的神经网络首先将低分辨率图像插值放大然后再进行生成的方法。我们用了一个叫做 robust Charbonnier 的损失函数（百度、Bing对于这个损失函数只甩出了它的论文，一点介绍都木有）。然后我们在金字塔层之间的递归层中共享了一部分参数，因此减少了参数量。在测试数据集中与state-of-art的不相上下的结果，但是！我们快。 主要贡献首先还是不用读之目前的论文都是垃圾系列，为了解决这些垃圾论文的问题，我们提出了一系列的方法： 精准：与之前论文直接插值放大图像然后进行重建不一向，我们是用卷积层和上采样滤波器逐层放大，这个结构可以感知更多的信息。 快速：我们相比于以cnn为基础的网络都要快，就像FSRCN一样的效果（还有这种引用论文的操作？） 改进的结构：首先，我们在金字塔各级之间共享参数，减少了73%的参数并且保证了更好的效果；其次，我们运用了类似残差的网络结构，使得我们能够搭建84层的神经网络；最后，我们在一个模型中实现了不同尺度的放大，我们称之为MS-LapSRN 详细来说接下来我们将从网络结构、参数共享、损失函数、多尺度模型和网络训练这几部分进行介绍。 神经网络模型结构我们的模型建立在拉普拉斯金字塔结构的基础上，输入为低分辨率图像，而不是插值填充后的图像。模型由两个部分组成，特征提取和图像重建。 上图中（a）是首先进行上采样，然后进行卷积等操作，（b）首先进行一系列操作提取特征，然后再进行上采样，（c）是我们论文提出的方案，红线是卷积层，蓝线是上采样层，绿色是Element-wise层，所以我们的模型由两个分支组成，同步进行。 特征提取分支：通过卷积等建立非线性映射，然后上采样得到图。 图像重建分支：首先进行上采样，然后把特征提取得到的图直接Element-wise add到这个图中，也就相当于两个图进行融合。 和LAPGAN不同的点是:我们的图是上采样后直接传下来，LAPGAN是上采样再经过卷积以后再传下来。至于为什么这样做，原文的话是这样的：Our network design effectively alleviates the computational cost and increases the size of receptive fields,大概意思是减少了计算量并增加了感受野，我是没能理解这其中的道理。 参数共享由于逐级放大过程中每一个放大块的网络结构都是一样的，所以我们共享了feature embedding、上采样、卷积的参数，节省了大量的参数和计算过程。 上图中是详细的网络结构，每一个放大块都是由feature embedding、上采样（反卷积层）、卷积层组成的，每块的结构相同，参数值很接近，所以我们共享了这部分的参数，也就是每个块的参数都一样。 为了得到更深的卷积神经网络、避免梯度爆炸的问题，我们还是采用了残差网络，不细说了，大家都一样。 损失函数我们舍弃了L2和L1这两种损失函数，因为他们都会造成图像过于平滑的问题，我们采用的损失函数是一个很稳定的损失函数 Charbonnier loss function，总体的函数我直接截图了，论文中也并没有提及这个损失函数为什么好用，感兴趣的可以去翻这个损失函数的论文。 英文部分是公式中参数的解释，很好懂，不翻译了（主要是因为不会在markdown中写公式[捂脸]）。 多尺度训练这里的多尺度在我的理解看来就是用逐级放大，每个都扩大2倍，而不是采用先用插值放大到单尺度的图像再更改图像信息。多尺度是什么意思呢？在我看来就像金字塔一样，逐级放大，放大2倍，4倍，8倍等等，每一个放大倍数就是一个尺度。 最后谈谈接下来就是模型训练参数设置细节什么什么的，不提了。放出结果看看 看起来还是不错，这只是生成图像的质量，在速度方面还要快很多，由于参数共享的原因，模型参数量大幅下降。其实这篇文章在我看来干货很多，因为逐级放大这个事儿跟我的思路很像，我是苦于没有找到合适的方法进行描述，这下知道方法了。还有一个很重要的是参数共享，可以看到参数共享节省了大量的参数，模型就会很小。我对参数共享这个保留意见，不知道不共享参数的话会不会好很多。 总而言之，这是我最近读到的最复杂的一篇论文，复杂的结构让我很安心，太简单了总觉得不靠谱，可能这是病吧。 最后还是 LOVE AND PEACE]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>Super-resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【读论文】EDSR]]></title>
    <url>%2F2017%2F10%2F20%2Fthe-understand-of-EDSR%2F</url>
    <content type="text"><![CDATA[让我们跟上时代的节奏，看一下来自思密达的一篇赢得了NTIRE2017超分辨率挑战赛第一名的论文。 这篇论文名字很耿直，叫《增强深度残差网络单幅图像超分辨率》，言简意赅，跟外面的妖艳贱货根本不一样好伐。我们直接放出来实验结果（论文中的）来看一下这篇代表了目前最棒效果的论文怎么样。 还是很震撼的有木有，喵的胡须丝丝分明，放大4倍后的效果很震惊，这到底是怎么做到的？ 摘要看一下摘要，超分辨率最近的研究已经进入深度卷积神经网络（DCNN）的时代，残差网络表现尤为出色，我们在这篇文章中提出了一个增强超分辨率网络（EDSR），达到了state-of-the-art的效果。我们的模型表现如此优秀是由于我们移除了在卷积残差网络中一些不必要的模块,并且我们扩大了模型尺寸以便于进行稳定的训练。 提出方法近期的一些深度神经网络模型在信噪比指标上有很好的表现，然而这样的神经网络有很多问题，首先超分辨率重建效果对网络架构非常敏感，一些细小的变化对最终的结果产生了很大的影响，并且同一个网络架构在初始化和训练技术上不同最终会产生不同的结果，不太稳定，其次对于不同尺度的超分辨率问题之间没有进行联系，其实不同尺度的超分辨率重建之间有很多的关系。后来我们发现SRResNet节省了训练的时间和内存并且有着不俗的表现，我们决定搞一下残差网络（ResNet），原始的残差网络用于解决图像分类啊，检测啊这些高层的计算机视觉问题，因此，残差结构应用于涉及到像素这种底层问题并不适用，但是我们就是要搞。我们分析了一下残差网络，然后去除掉了一部分不需要的结构，简化了网络结构，然后小心翼翼的炼丹，最后果然发现练出了更好的效果，以至于取得了NTIRE 2017超分辨率挑战的第一第二名。接下来看我们是怎么做的。 残差块 (Residual blocks)像SRResNet已经将残差网络用于超分辨率了，我们还是改进了一下。 我们去掉了BN层（batch normalization layers），这个层吧，在分类问题中很好用，但是在超分辨率问题中真的并没有什么卵用，去掉它我们节省了40%的内存（相较于SRResNet），然后我们就可以用这部分内存构建一个更大的模型来得到更好的表现，毕竟我们的计算资源有限。 单尺度模型（Single-scale model）增强网络模型表现最简单的方法就是增加神经网络的参数。在卷积神经网络中可以通过堆叠更多的卷积层和滤波器个数，但是我们发现随着特征图（feature maps）的增加，神经网络的训练会变得很不稳定，于是我们想用残差不就完了么，什么,SRResNet用过了？我们去掉一个激活函数层！（写到这儿，我只有一个表情[白眼]），并且我们用了一个提前训练的网络层作为起始状态，而不是随机初始化，我们得到了一个很好的表现结果（写到这儿，我只有一个表情[????]，fine-tune当然快啊！！），我不想放论文的图了，这图是单纯的凑数啊。 我还是放出来给大家看一下吧。 多尺度模型（Multi-scale model）fine-tune会提高最终的结果，我们想这是为什么呢，莫不是提前训练（pretrained）的模型与当前模型之间有什么关系？既然他们有一些说不清楚的关系，我们把他们放在一起好了，于是我们提出了一个多尺度的模型，他们共享中间的残差块的参数，这样我们对于不同的尺度（也就是放大倍数）就不用单独的构建网络模型了，节省了很多的参数（谁来告诉我这是什么逻辑）。 没错就是这个样纸，我们把三个模型合成了一个，然后共用了一部分参数（这是什么啊，摔！）。 总结实验过程咱就不说了，就是一些网络模型参数和训练参数，这篇论文看下来最大的贡献就是通过实验证明了BN层在超分辨率问题上没有用，但是这也能写篇论文？？！！当然，思密达也意识到了这个问题，于是他们加了很多废话。整篇论文就是这样，至于为什么去掉BN层效果就会提高很多呢？神经网络大黑盒，谁又说的清楚。 最后附上论文地址：https://arxiv.org/pdf/1707.02921v1.pdf LOVE AND PEACE]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>Super-resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【读论文】CapsNet]]></title>
    <url>%2F2017%2F10%2F20%2Fthe-understand-of-Capsnet%2F</url>
    <content type="text"><![CDATA[Hinton老师打算推翻这个世界 最近所有科技自媒体都把目光放在了Hinton老师的一篇文章《Dynamic Roution Between Capsules》上，其实这篇文章在去年十一月份就发出来了，最近这篇文章的一作开源了论文的源代码，又引起了一波狂欢。科技自媒体的点在于Hinton提出了一种Capsules结构，来推翻自己提出的深度学习的基础–反向传播算法，并且直译为“胶囊网络”，简直不忍直视，说好的信达雅呢。但是我认为，这篇文章并没有完全的推翻BP算法，提出了一个新的角度来解决问题，来试图尝试解释神经网络。这篇文章的意义目前还不明显，因为Hinton老师确实给大家挖了一个大坑来填，论文中还有一些想法目前很粗糙。港真，可以水论文了。 Capsules论文提出了一种叫做capsules的结构，capsules是一个向量，表征了两个部分： 其长度表征了某个实例（物体、视觉概念或者是他们的一部分）出现的概率 其方向（长度无关的部分）表征了物体的某些图像属性（位置、颜色、方向、形状等）既然capsules是一个向量，那么它是如何被应用呢？capsules作为一个向量，我们可以把它理解为feature map中的一个点。目前我们在神经网络中传递的数值都是标量，通过最后的Loss值，然后反向传播用来更新权重。capsules概念的提出让我们可以试图通过更容易理解的方式来构建神经网络，后面再提。在我看来，capsules的结构更像是传统神经网络中的一个层，因为论文后面提出的CapsNet也是以CNN开头的。后面的级联结构只是把传递的标量变为向量。 既然标量变成了向量，我们就不能用传统的非线性激活函数来处理，论文中提出了一个被称为squashing的非线性函数作为激活函数【公式】【图】这个函数的特点就是： 值域在[0,1]之间，所以输出向量的长度可以表征某种概率，我们就可以用它做分类； 函数单调递增，原来较长的向量收到鼓励，较短的向量收到抑制。squashing函数实际上做到了对capsules的压缩和重新分布。 与此同时，层与层之间的连接部分也做出了相应的更改，变成了线性组合+Dynamic Routing的组合，这不就是变相的全连接层嘛，与全连接不同的是，这个组合针对的是一堆capsules。【图】Ci是可以进行更新的参数，这也是为什么被称为动态的原因所在，Routin的更新机制依赖于自己的输出，更新算法如下【图】更新算法很容易收敛，论文中认为3次迭代足矣。routing和其他算法一样存在过拟合，迭代次数过多能够提升准确率，但也会影响泛化能力。 CapsNetCapsNet的结构示意图如下，一共只有三层网络，CNN + PrimaryCaps + DigitCaps，论文最后的结果可以看到在MNIST数据集上可以媲美很深的卷积神经网络，然而，只是在MNIST上，其他数据集中表现并不好，所以还有许多坑要填。【图】首先还是一个卷积核为9x9的卷积层，卷积核的尺寸很大，应该是为了更充分的感受到原始图像。为什么不从一开始就用capslues？有一种说法是capslues并不能从像素级别提取特征，需要一个浅层的卷积用来提取特征，然后再进行进一步的归纳特征，好像也是有那么个意思。 然后是PrimaryCaps层，在卷积层中用8个不同的卷积核组成，一共是32个feature map。 最后一层是DigitCaps层，也就是输出层，一组10个标准的Capsules，每一个capsule代表一个数组，输出向量元素的个数是16个。 只有在PrimaryCasules到DigitCaps中使用了上文所说的Dynamic routing。 Reconstruct看到这里，会觉得这也没有啥很特殊的，也就是一个特殊的结构而已嘛，接下来的工作就是真正神奇的地方了。 Hinton一直坚持的一个理念就是，一个好的robust的模型，一定能够由重构能力，能够通过提取出的特征表示还原出原始图像，我们才能够说这个模型能够很好的表示特征。 论文中把DigitCaps的数据直接扔到全连接层进行重构，如下图的结构。【图】论文中通过人为扰动capsule的输出向量能够成功的对图像的特征进行有规律的改变，这是以往的网络结构，包括GAN中很难做到的一点。【图】甚至对于重叠的数字，都会由很强的重构能力。[tu]我们有理由相信，Capsules结构中包含了我们能够理解的符合人类习惯的信息，我们可以通过人为的干扰改变重构信息，我们就多了一丝机会去理解神经网络。 Discussion至于这项工作是否值得像科技媒体那样的震惊，我认为目前还欠火候。首先并没有完全的抛弃反向传播算法，第一层的CNN的参数还需要更新，其次，Dynamic Routing的参数量问题是一个不得不考虑的事情。Hinton大神表示，坑已经挖好，你们可以灌水了。至于这个究竟是不是真的会带来改变，只能说，时间，时间会告诉我答案。]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【读论文】SRGAN]]></title>
    <url>%2F2017%2F10%2F12%2FThe-understand-of-SRGAN%2F</url>
    <content type="text"><![CDATA[这篇论文第一次将GAN（生成式对抗网络）引入图像超分辨率，取得state-of-art的效果，论文的效果值得称赞！ 论文是2016.09.15发布在arXiv上，作者栏上Twitter很是显眼，低头看了一眼还在显示娱乐八卦的微博，反手就是一巴掌… 综述这篇论文摘要中说，最近的图像超分辨率的工作大都集中于以均方差（MSE）作为损失函数，这样会造成生成图像过于平滑，缺少高频细节，看起来很不爽，所以提出了基于生成式对抗网络的网络结构，据我们所知这是生成式对抗网络第一次应用于4倍下采样图像的超分辨重建（无形装逼，最为致命）。本文提出了一个由adversarial loss和content loss组成的损失函数，损失函数作为GAN的判别器损失函数来对生成图像进行判别。多说一句，我们的损失函数是基于感知相似性而不是像素空间的相似性（不要用信噪比（PSNR）那种东西来衡量我的论文），所以我们的生成图像的质量非常好。 主要贡献论文刚开始不出所料的对各路大神在超分辨率的工作进行无情嘲讽，就按下不表了，主要说说贡献。这边文章最大的贡献是将生成式对抗网络应用于图像超分辨率，让判别器分辨生成的图像和真实的图像，这样就能达到photo-realistic的效果，这个想法不错，然后设计了一个新的损失函数进行判别我们接下来会详细的说明一下。 对抗式生成网络结构生成式对抗网络我们就不详细说了，感兴趣的同学们可以去看下一以Goodfellow为首的众大神的一系列文章 在这篇论文中的GAN生成器式这么描述的:At the core of our very deep generator network G,which is illustrated in Figure are B residual blocks with identical layout. Sepecifically, we use two convolutional layers with small 3x3 kernels and 64 feature maps followed by batch-normalization layers and ReLU as the activation fuction. We increase the resolution of the input image with two trained deconvolution layers.原文是这样大家也能看得懂，不翻译了,大概就是残差块+卷积层+BN层+ReLU，对于GAN的判别器就是VGG+LeakyReLU+max-pooling,具体结构如图。 “感知”损失函数感知损失函数的也是本文巨大创新点之一，大概就是不从像素层面判断生成图像的质量好不好，而是从观感的角度来判别，由判别器来判别。感知函数由两个部分组成： Content Loss Adversarial Loss 公式分别如下： 组合体公式如下： 公式背后的数学意义大概就是MSE+GAN，每个占一定部分的权重，分别表示空间的相似性、判别器看到的相似性。具体的公式细节大家自行感受一下。 展望未来当然，论文最后展望了一下未来，认为PSNR并不能代表生成图像的质量（毕竟这篇文章的得分不是很高[坏笑]），我们要找一个更好的衡量方法！但是从实验结果来看，确实这样，人的感官不能单靠信噪比来表示，数学意义上的相似并不代表感官上的相似。我们还要把网络做的更深你怕不怕，我们还要找更好的损失函数你怕不怕。 在论文后面放出实验结果 最后 LOVE AND PEACE 更新：https://arxiv.org/abs/1609.04802 论文地址，我写完才发现这篇论文在2017年5月又提交了一版，删除了正则化的损失函数，其实本来正则化的系数就很小，我把正文也修改了，Twitter你这个善变的社交网站。]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>Super-resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【图像超分辨率】简介]]></title>
    <url>%2F2017%2F10%2F07%2Fsuper-resulution-1%2F</url>
    <content type="text"><![CDATA[硕士开题最终选择基于深度学习的图像超分辨率作为课题，图像超分辨率也是个人觉得在深度学习层面一个能够扩展到工业界的一个很有效的切入点，所以写下一系列的博文对图像超分辨率问题进行一个很详细的解读。本文作为此系列的博客的开头，首先对超分辨率这一问题进行大体的说明。 图像超分辨率通俗的解释来讲，就是通过各种方法把一个或者一组低分辨率的图像扩展到高分辨的图像。对于分辨率大家可能很熟悉，在各大视频APP中，都会有视频清晰度的选项，普清、高清、超清、1080p等等，这些就是对应的图像分辨率，图像分辨率越高，我们眼睛接受到的信息量越大，对于细节的描述就更清楚，我们也能得到更好的感观体验。 Harris和Goodman分别于1964年和1968年分别提出图像分辨率复原，但是并没有被世人所重视，一直到Tsai提出低分辨率序列复原单帧高分辨率图像以后，超分辨率重建技术才开始得到日益广泛的研究。时至今日，超分辨率依然是计算机视觉领域非常火热的研究方向。]]></content>
      <categories>
        <category>图像超分辨率</category>
      </categories>
      <tags>
        <tag>Super-resolution</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在hexo博客中插入图片的方法]]></title>
    <url>%2F2017%2F10%2F04%2F%E5%9C%A8hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在markdown文件中插入图片是一件巨麻烦无比的事情，特别是在图片很多的情况下，先把图片上传到服务器，再把图片的链接贴在markdown文件中，这个过程可以吃下三个月饼、两个香蕉，经过寻找，终于发现了一个快捷、方便、好的不行的插入图片方法，直接在本地添加。 1.首先在站点目录下的_config.yml中是否存在post_asset_folder:这句话，把冒号之后的false改为true，hexo默认的参数是false。 2.在blog文件夹下执行1npm install https://github.com/CodeFalling/hexo-asset-image --save 默默的等它跑完。 3.接下来就可以正常的开始新建一个文章：hexo new 文章名字,你发现在 /source/_posts/ 目录下会出现一个新的md文件和一个同名的文件夹，图片就可以放在这个文件夹的下面。12345/name 1.jpg 2.jpg 3.jpgname.md 4.在文章中只要使用![logo](name/1.jpg)既可以插入图片，[]中的logo是图片的名字。 这样做的好处是方便，不好的地方是图片真的很占地方，所以酌情使用。 插一张图片试试效果]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从源码编译TensorFlow]]></title>
    <url>%2F2017%2F04%2F18%2F%E4%BB%8E%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91TensorFlow%2F</url>
    <content type="text"><![CDATA[最近接到一个深度学习用于关键点定位的项目，开始应用深度学习进行图像的识别。考察了现有的几个深度学习框架，无疑在图像领域Caffe还是最专业的一个深度学习框架。但是Caffe的入门比较困难，后来看到Github上TensorFlow的项目比较多。时间紧，任务重，好入门就是王道。所以毅然决然的踏入了TensorFlow的坑。这一系列的文章将会贯穿我整个项目过程，打上TensorFlow的标签进行记录。本文配置基于Ubuntu16.04LTS，Windows用户出门左转。 1、安装依赖库TensorFlow的安装需要依赖库，包括一些必要的运行环境。装上都用的到，也不占多大地方，强迫症自行解决。 sudo apt-get install -y build-essential git python-pip libfreetype6-dev libxft-dev libncurses-dev libopenblas-dev gfortran python-matplotlib libblas-dev liblapack-dev libatlas-base-dev python-dev python-pydot linux-headers-generic linux-image-extra-virtual unzip python-numpy swig python-pandas python-sklearn unzip wget pkg-config zip g++ zlib1g-dev sudo pip install -U pip 2、配置CUDA和Cudnn这两个库的配置参考该官方网站，配置过程简单不在赘述。 http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4VZnqTJ2A https://developer.nvidia.com/cudnn 从官方网站下载好cuda和cudnn以后用以下命令安装。 dpkt -i cuda文件名 sudo apt update sudo apt install cuda cudnn配置的过程简而言之就是把下载好的cudnn复制到相应的cuda库中，下载速度会有限制，自行科学上网。因为要复制到/usr/local/cuda-8.0中，所以要用命令行给root权限。 注意！ 接下来需要配置环境变量。打开一个终端。 cd gedit .bashrc 然后将如下代码复制粘贴在末尾。 export CUDA_HOME=/usr/local/cuda-8.0 export CUDA_ROOT=/usr/local/cuda-8.0 export PATH=$PATH:$CUDA_ROOT/bin export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_ROOT/lib64 然后使环境变量生效。 source .bashrc 3、安装JAVA8sudo add-apt-repository ppa:webupd8team/java sudo apt-get update sudo apt-get install oracle-java8-installer 安装过程有图形化界面，在许可证问题上选Yes，可以通过Tab键进行选择。 4、安装bazelecho &quot;deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8&quot; | sudo tee /etc/apt/sources.list.d/bazel.list curl https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg | sudo apt-key add - sudo apt-get update &amp;&amp; sudo apt-get install bazel sudo apt-get upgrade bazel 这一步可能需要科学上网，否则速度会很慢从而导致失败 5、源码编译TensorFlow在合适的位置打开终端，从github上下载Tensorflow仓库。 git clone https://github.com/tensorflow/tensorflow 完成之后，在打开其文件夹，然后开始进行配置。 cd tensorflow ./configure 注意： 1、只需要将选项 build tensorflow with cuda support 之后写y，其他选项都直接回车，默认不支持。 2、CUDA SDK version 写 8.0。 3、Cudnn version 写5。 4、Compute capability 写 6.1。（这里的值需要自己查询电脑GPU型号对应的计算能力，我的电脑是GTX1080，所以对应6.1） 6、生成pip安装包bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 从第二行命令可以看出来，生成的.whl安装包将会放置在/tmp/tensorflow_pkg 7、安装pip安装包sudo pip install /tmp/tensorflow_pkg/tensorflow-1.0.1-py2-none-any.whl .whl的名字是不固定的，安装的时候要看 /tmp/tensorflow_pkg/文件夹下具体是什么名字。至此安装过程全部完成。 8、多说一段本次的安装过程是进行GPU版的TensorFlow的安装。这样就不再用在程序中写代码指定GPu了，直接会启用GPU。非GPU版的安装直接用pip安装即可 sudo pip install tensorflow TensorFlow是一个强大的开源深度学习框架，就目前做的工作来看，深度学习对于浅层网络还是有绝对的优势。但是深度学习依赖于大数据，人工标注数据成本很高，不过会有各种新的模型出现以解决这个问题吧。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序]]></title>
    <url>%2F2017%2F04%2F16%2Fxu%2F</url>
    <content type="text"><![CDATA[拖拉拉几个月，终于还是搭建起来这个博客。 看了很多文章，也废了很多时间，决定写一些东西，以此记录下来工作学习感想。 这个博客里面会记录下这段时间内的学过的，想过的，做过的。 谨以此献给支持我的家人同学以及奕葶小朋友。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
